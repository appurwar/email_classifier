#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Fri Nov  3 17:03:36 2017

@author: apoorv
"""


import nltk
from nltk.probability import FreqDist
from nltk.tokenize import word_tokenize
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score
from sklearn import svm, tree
from sklearn.neural_network import MLPClassifier

import glob
import errno
import string
import sys
import os
import csv
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import CountVectorizer
import re

def atoi(text):
    return int(text) if text.isdigit() else text

def natural_keys(text):
    return [ atoi(c) for c in re.split('(\d+)', text) ]

def preprocess(file_list):
    flat_list = list();
    raw_documents = []
    translator = str.maketrans('', '', string.punctuation)
    porter = nltk.PorterStemmer()
    wnl = nltk.WordNetLemmatizer()
    
    for name in file_list: # 'file' is a builtin type, 'name' is a less-ambiguous variable name.
        try:
    #       print(name)
           with open(name,'rb') as f:
            flat_list = []
            document = ''
            for line in f:
                line = line.strip()
                line = str(line)
                split = line.split()
                
                if split:              
                    flat_list = line.split()
                    long_words = [w for w in flat_list if len(w) > 2]
                    
                    long_words_noPunc = [w.translate(translator) for w in long_words]
                    
                    long_words_lower = [x.lower() for x in long_words_noPunc]
                    
                    stops = set(stopwords.words("english"))   
                    meaningful_words = [w for w in long_words_lower if not w in stops]  
                    meaningful_words = ' '.join(meaningful_words)
                  
                    document = document + ' '  + meaningful_words
                    
            raw_documents.append(document)
            
        except IOError as exc:
            if exc.errno != errno.EISDIR: # Do not fail if a directory is found, just ignore it.
                raise # Propagate other kinds of IOError.
    return raw_documents



spam_train_path = '/Users/apoorv/MyDocs/CU Courses/Fall 2017/ML/HW/HW3/email_classification_data/train_data/spam/*.txt'
ham_train_path = '/Users/apoorv/MyDocs/CU Courses/Fall 2017/ML/HW/HW3/email_classification_data/train_data/ham/*.txt'   

test_path = '/Users/apoorv/MyDocs/CU Courses/Fall 2017/ML/HW/HW3/email_classification_data/test_data/*.txt'

spam_train_files = glob.glob(spam_train_path)
ham_train_files = glob.glob(ham_train_path)   

all_test_files = glob.glob(test_path)
all_test_files.sort(key=natural_keys)

all_train_files = []
all_train_files.extend(spam_train_files)
all_train_files.extend(ham_train_files)


#0 - ham
#1 - spam
train_labels = []
train_labels.extend([1] * len(spam_train_files))
train_labels.extend([0] * len(ham_train_files))


vectorizer = CountVectorizer(analyzer = "word",   
                             tokenizer = None,    
                             preprocessor = None, 
                             stop_words = None,   
                             max_features = 5000) 

raw_train_documents = preprocess(all_train_files)
X_train_tfidf = vectorizer.fit_transform(raw_train_documents)
print(X_train_tfidf.shape)


raw_test_documents = preprocess(all_test_files)
#print(raw_test_documents)
X_test_tfidf = vectorizer.transform(raw_test_documents)
print(X_test_tfidf.shape)

#clf = MultinomialNB().fit(X_train_tfidf, train_labels)
#predictions = clf.predict(X_test_tfidf)

#clf = svm.SVC(C = 0.1, kernel='poly', degree=6)
#clf.fit(X_train_tfidf, train_labels)

#clf = tree.DecisionTreeClassifier()
#clf.fit(X_train_tfidf, train_labels)

clf = MLPClassifier(activation='relu', solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(70,2), random_state=1)
clf.fit(X_train_tfidf, train_labels)

csvFile = open('/Users/apoorv/MyDocs/CU Courses/Fall 2017/ML/HW/HW3/email_classification_data/results.csv', 'w') 
topRow = "email_id,labels\n"
csvFile.write(topRow)
    

predictions = clf.predict(X_test_tfidf)

i=1
for x in predictions:
    row = str(i) + ',' + str(x) + '\n'
   # print(row)
    csvFile.write(row)
    i = i+1

csvFile.close()

print(predictions)
#print(accuracy_score(predictions, test_labels))

